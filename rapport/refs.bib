
@inproceedings{ScienceQA,
  title     = {Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
  author    = {Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Ashwin Kalyan},
  booktitle = {The 36th Conference on Neural Information Processing Systems (NeurIPS)},
  year      = {2022}
}

@online{lora,
  author     = {Edward J., Hu and Yelong, Shen and Phillip, Wallis and Zeyuan, Allen-Zhu and Yuanzhi, Li and Shean, Wang and Lu, Wang and Weizhu, Chen},
  title      = {Lora: Low-rank adaptation of large language models},
  year       = {2021},
  eprinttype = {arxiv},
  doi        = {https://doi.org/10.48550/arXiv.2106.09685}
}

@online{qlora,
  author     = {Tim, Dettmers and Artidoro, Pagnoni and Ari, Holtzman and Luke, Zettlemoyer},
  title      = {QLoRA: Efficient Finetuning of Quantized LLMs},
  year       = {2023},
  eprinttype = {arxiv},
  doi        = {https://doi.org/10.48550/arXiv.2305.14314}
}

@misc{ragreview,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10997}, 
}

@article{Qwen2VL,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@inproceedings{liu2023llava,
  author      = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  title       = {Visual Instruction Tuning},
  booktitle   = {NeurIPS},
  year        = {2023}
}

@article{chen2023internvl,
  title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
  journal={arXiv preprint arXiv:2312.14238},
  year={2023}
}

@online{dora,
  author     = {Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
  title      = {DoRA: Weight-Decomposed Low-Rank Adaptation}, 
  year       = {2024},
  eprinttype = {arxiv},
  doi        = {https://doi.org/10.48550/arXiv.2402.09353}
}

@misc{valizadehaslani2022twostagefinetuningnovelstrategy,
      title={Two-Stage Fine-Tuning: A Novel Strategy for Learning Class-Imbalanced Data}, 
      author={Taha ValizadehAslani and Yiwen Shi and Jing Wang and Ping Ren and Yi Zhang and Meng Hu and Liang Zhao and Hualou Liang},
      year={2022},
      eprint={2207.10858},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2207.10858}, 
}

@misc{idefics2,
      title={What matters when building vision-language models?}, 
      author={Hugo Laurençon and Léo Tronchon and Matthieu Cord and Victor Sanh},
      year={2024},
      eprint={2405.02246},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.02246}, 
}

@misc{huang2024llm2clippowerfullanguagemodel,
  title={LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation},
  author={Weiquan Huang and Aoqi Wu and Yifan Yang and Xufang Luo and Yuqing Yang and Liang Hu and Qi Dai and Xiyang Dai and Dongdong Chen and Chong Luo and Lili Qiu},
  year={2024},
  eprint={2411.04997},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2411.04997}
}

@article{timoneda2024bert,
  author = {Timoneda, Joan C. and Vallejo Vera, Sebastián},
  title = {BERT, RoBERTa, or DeBERTa? Comparing Transformer Models in Political Science Text},
  journal = {Journal of Politics},
  year = {2024},
  note = {Forthcoming},
url={https://svallejovera.github.io/files/bert_roberta_jop.pdf}
}


