\documentclass[11pt]{article} % Type of document and font size
% \renewcommand{\baselinestretch}{1,15} % Specify line spacing
\usepackage[margin=2.5cm]{geometry} % Précise les marges du document
\usepackage[utf8]{inputenc}	% Précise comment le texte est saisi : cela permet de tapper directement les accents
\usepackage[english]{babel} % For English
% \usepackage[T1]{fontenc}	% Précise la façon dont le document actuel est encodé
\usepackage{setspace}
\usepackage{indentfirst} % To indent the first paragraph
% \usepackage{natbib} % To have bilbiography
\usepackage{csquotes}
\usepackage[sorting=none]{biblatex} % Bibliography
\addbibresource{ref.bib}



%Autres packages et commandes utiles
%----------------------------------------------------------------
\usepackage{enumerate} % Pour mieux gérer la commande enumerate dans les sections
\usepackage{graphicx}	% Pour inclure des images
\graphicspath{ {./images/} } % set path to images
\usepackage{hyperref} % to allow hyperlinks
\usepackage{typed-checklist} % for checklist

%Titre
%----------------------------------------------------------------
\title{IFT-7030 Project proposal}
\author{Philippe Gélinas 536 782 326}
\date{\vspace{-5ex}} % Don't show date and remove spacing

%----------------------------------------------------------------
\begin{document}

\maketitle

\section*{Abstract}
The project aims to compare different approches to fine-tune a multimodal large language modal (MLLM) for multimodal multiple-choice question answering on the ScienceQA dataset \cite{ScienceQA}. The first method is a two-step fine-tuning method which aims to increase the reasoning ability of a MLMM and the second method uses RAG framework to extend the knowledge of the MLMM outside of its training data.

\section*{Introduction}
We will compare different ways to fine-tune an MLMM for multimodal multiple-choice question answering. In this type of task, a multiple-choice question is asked which references a given image. We will use the ScienceQA dataset as a benchmark. This dataset contains primary and high-school questions related to science and is easily available on HuggingFace.

The first fine-tuning method will consist of a two-step fine-tuning method. Since the ScienceQA dataset contains a relatively small amount of data for NLP tasks (21k observations), we will first try to improve the reasoning abilities of the MLMM by performing supervised fine-tuning. We will use a larger dataset for this such as SearchQA (a collection of 140k Jepoardy! questions), HotpotQA (113k reasoning questions from Wikipedia articles), or Orca-Math (a dataset containing roughly 200k grade school math problems). After this first round of fine-tuning, the MLLM will then be fine-tuned to specialize in multiple-choice question answering with the ScienceQA dataset using regular fine-tuning. Regular fine-tuning consists of freezing all but the last layer and updating weights.

Another fine-tuning approach we will explore is QLoRA. This deviate from regular fine-tuning by representing the weight update matrix ($\Delta W$) as a low-rank decomposition of the weight matrix.

If we have time, we will compare with a training approach using retrieval-augmented generation (RAG). RAGs can help improve the capabilities of LLMs to specific domains by referencing knowledge outside of the training sources of the LLM.


\section*{Literature review} 

LoRA (Low-Rank Adaptation) is a fine-tuning technique for LLMs introduced by \cite{lora}. It deviates from regular fine-tuning by freezing the original model weights and by updating a seperate set of weights which are then added to the orginal weights. In regular fine-tuning an entire wieght update matrix ($\Delta W$) is combined with the pre-trained weights. LoRA seperates $\Delta W$ into 2 low-rank matrices that approximate it. This method significantly reduces the number of trainable parameters. Along with the reduced computational load this brings, it can also help prevent overfitting. QLoRA \cite{qlora} is a modification of LoRA that introduces increased memory efficiency due to storing weight parameters with 4-bit precision. \cite{qlora} observed performance levels similar to those of LoRA.

\section*{Input-Output Dataset}

As mentioned, we will use the ScienceQA dataset. The ScienceQA dataset contains 21\,208 multimodal multiple-choice questions collected from primary and high school curricula. The questions are separated into 3 subjects: natural science, language science, and social science. Roughly 49\,\% of the questions have an image context, 48\,\% have a text context, and 31\,\% have both. An interesting aspect of the ScienceQA dataset is that most question contain lectures (84\,\%) and detailed explanations (91\,\%) that help answer the question. The dataset has already been split into train, test, and validation sets on HuggingFace. The train set contains 12\,726 observations, whereas the test and validation sets each contains 4\,241 observations. We will use the train and tests sets to fine-tune the model and the test set to evaluate performance. The following diagram illustrates what the input (query) can look like and how it will be processed when using RAG : 

\includegraphics{RAG_diagram.pdf}
\begin{enumerate}
    \item Embed the query with the embedding model to represent it as a vector.
    \item Pass the query embedding to the vector database of the RAG.
    \item Retrieve the $n$ most relevant contexts from the external documents. The relevant contexts are found using distance from the query embedding and the embedded chunks in the vector database. 
    \item Pass the original query and the retrieved contexts to the LLM.
    \item Obtain response from the LLM. The response will be the index of the chosen answer from the list of choices.
\end{enumerate}

When using the fine-tuned LLM without RAG, the process will be much simpler. The query will simply be passed to the LLM and an answer will be outputted.

We will use a pretrained LLM as the base model. We must use a multimodal LLM as some queries can contain images. COMPLETE THIS PARAGRAPH.

\section*{Performance Metrics}
We will evaluate performance using accuracy. It is a widely used and suitable metric for multiple-choice question answering since it is simple and easily interpretable.

\section*{Computational Resources}
Fine-tuning an LLM requires good GPU resources to have reasonable trainig times. Given that the ScienceQA dataset is not extremely big and that we will limit ourselves to an LLM with no more than 8b parameters, the resources Valeria provides should be adequate. As a backup, our team has access to an NVIDIA RTX 4060 laptop GPU with 8 gigabytes of VRAM which should still manage to fine-tune a smaller sized LLM if trained overnight. 

\section*{Checklist}
\begin{CheckList}{Task}
    \Task{done}{Realistic} The dataset is being used is well defined and will require little pre-processing. The bulk of the time will be spent fine-tuning an LLM. With access to GPUs from Valeria and the ability to execute training for long periods, it is feasible to fine-tune an LLM multiple times.
    \Task{done}{Well defined dataset} The ScienceQA dataset is available on HuggingFace. Each question has an identified correct answer and, when applicable, an associated image and context.
    \Task{done}{Clearly define input and output} The input the LLM will receive is a multiple-choice question along with a list of possible answers. The output will be an index of the answer list that corresponds to the chosen answer.
    \Task{done}{Not be trivial} LLM fine-tuning is far from being a trivial task. LLMs are still not fully understood and research is still being done to optimize their fine-tuning. 
    \Task{done}{Within machine learning and/or signal processing domain} NLP tasks such as QA lie within the signal processing domain. Using a transformer model to solve the task also means that it lies within the machine learning domain.
    \Task{done}{Well situated within the literature} Many articles have been published on the topic of fine-tuning LLMs. It is still an active research subject and new fine-tuning methods are constantly being explored.
\end{CheckList}

\medskip
\printbibliography

\end{document}