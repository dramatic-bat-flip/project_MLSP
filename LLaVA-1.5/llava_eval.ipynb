{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will evaluate the performance of the Idefics models on the dataset. This can be used to evaluate any of the fine-tuned Idefic2 models we trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the ScienceQA dataset from HuggingFace. We only need the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\philg\\anaconda3\\envs\\cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Value\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ds = load_dataset(\"derek-thomas/ScienceQA\")\n",
    "ds = ds.filter(lambda example: example['image'] is not None) #only keep obs with images\n",
    "ds = ds.cast_column('answer', Value(\"string\")) #convert answer from int to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ds['test']\n",
    "test_dataset = test_dataset.remove_columns(['hint', 'task', 'grade', 'subject', 'topic', 'category', 'skill', 'lecture', 'solution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in the LLaVA-1.5 tokenizer and processor from HuggingFace. Then we upload the fine-tuned LLaVA-1.5 model from our local repository (if we want to evalute on another fine-tuned LLaVA model we just need to change this path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:32<00:00, 10.78s/it]\n",
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlavaForConditionalGeneration(\n",
       "  (vision_tower): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(577, 1024)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPSdpaAttention(\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=6, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=6, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict(\n",
       "                  (default): lora.dora.DoraLinearLayer()\n",
       "                )\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=6, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=6, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict(\n",
       "                  (default): lora.dora.DoraLinearLayer()\n",
       "                )\n",
       "              )\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=6, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=6, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict(\n",
       "                  (default): lora.dora.DoraLinearLayer()\n",
       "                )\n",
       "              )\n",
       "              (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): LlavaMultiModalProjector(\n",
       "    (linear_1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "    (act): GELUActivation()\n",
       "    (linear_2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
       "  )\n",
       "  (language_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32064, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaSdpaAttention(\n",
       "            (q_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=6, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=6, out_features=4096, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict(\n",
       "                (default): lora.dora.DoraLinearLayer()\n",
       "              )\n",
       "            )\n",
       "            (k_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=6, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=6, out_features=4096, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict(\n",
       "                (default): lora.dora.DoraLinearLayer()\n",
       "              )\n",
       "            )\n",
       "            (v_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=6, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=6, out_features=4096, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict(\n",
       "                (default): lora.dora.DoraLinearLayer()\n",
       "              )\n",
       "            )\n",
       "            (o_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=6, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=6, out_features=4096, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict(\n",
       "                (default): lora.dora.DoraLinearLayer()\n",
       "              )\n",
       "            )\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=6, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=6, out_features=11008, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict(\n",
       "                (default): lora.dora.DoraLinearLayer()\n",
       "              )\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=6, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=6, out_features=11008, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict(\n",
       "                (default): lora.dora.DoraLinearLayer()\n",
       "              )\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=11008, out_features=6, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=6, out_features=4096, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict(\n",
       "                (default): lora.dora.DoraLinearLayer()\n",
       "              )\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoTokenizer, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "# Load in the processor which formats inputs for LLaVa\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "# processor.tokenizer.padding_side = \"right\" # during training, one always uses padding on the right\n",
    "\n",
    "# We will us QLoRa, so we specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"LLaVA-1.5_qlora_ft/checkpoint-258/\", # change path for other llava ft models\n",
    "                                                      torch_dtype=torch.float16,\n",
    "                                                      quantization_config=quantization_config).to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the evaluation loop. The evaluation loop is pretty simple, we format the question and choices from the dataset into the correct format (sort of like the data collator) and then have the model infer the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "EVAL_BATCH_SIZE = 4\n",
    "\n",
    "answers_unique = []\n",
    "generated_texts_unique = []\n",
    "transform = transforms.Compose([transforms.PILToTensor()])\n",
    "\n",
    "for i in range(0, len(test_dataset), EVAL_BATCH_SIZE):\n",
    "    examples = test_dataset[i: i + EVAL_BATCH_SIZE]\n",
    "    answers_unique.extend(examples[\"answer\"])\n",
    "    images = [transform(im) for im in examples[\"image\"]]\n",
    "    texts = []\n",
    "    for q, c in zip(examples['question'], examples['choices']):\n",
    "\n",
    "        content = [{\"type\": \"text\", \"text\": \"Please read the multiple-choice question below carefully and answer only with an index of the given list of choices.\"}]\n",
    "        content += [{\"type\": \"image\"}]\n",
    "        content += [{\"type\": \"text\", \"text\": f\"{q}\\nThe choices are: {c}.\"}]\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content,\n",
    "            },\n",
    "        ]\n",
    "        text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        texts.append(text.strip())\n",
    "    inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
    "    generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    for text in generated_texts:\n",
    "        generated_texts_unique.extend(text.split(\"ASSISTANT:\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to modify the answers a little but to only conserve the index values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [g.strip().strip(\".\") for g in generated_texts_unique]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate the accuracy of the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8328223723081908\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def accuracy(preds, real):\n",
    "    p = np.array([int(x) for x in preds])\n",
    "    r = np.array([int(x) for x in real])\n",
    "    return sum(p == r)/len(real)\n",
    "\n",
    "accuracy(predictions, test_dataset['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of answers in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35349529003470503\n",
      "0.35994050570153696\n",
      "0.18591968269707487\n",
      "0.09667823500247893\n",
      "0.0039662865642042635\n"
     ]
    }
   ],
   "source": [
    "for i in (['0'], ['1'], ['2'], ['3'], ['4']):\n",
    "    print(sum(np.array(i) == np.array(test_dataset['answer']))/len(np.array(test_dataset['answer'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
